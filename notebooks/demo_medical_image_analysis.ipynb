{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Medical Image Analysis with Autoencoders and VAEs\n",
        "\n",
        "This notebook demonstrates the complete workflow for medical image analysis using the refactored codebase.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The refactored project provides:\n",
        "- **Data Processing**: DICOM/NIfTI loading, preprocessing, and efficient dataloaders\n",
        "- **Model Architectures**: 3D Autoencoders and Variational Autoencoders (VAEs)\n",
        "- **Training Infrastructure**: Comprehensive training loops with callbacks and checkpointing\n",
        "- **Analysis Tools**: Visualization, evaluation, and interpretation functions\n",
        "\n",
        "## Table of Contents\n",
        "1. [Setup and Imports](#setup)\n",
        "2. [Data Loading and Exploration](#data)\n",
        "3. [Model Training](#training)\n",
        "4. [Model Evaluation](#evaluation)\n",
        "5. [Analysis and Visualization](#analysis)\n",
        "6. [Advanced Usage](#advanced)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Setup and Imports {#setup}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Add project root to path\n",
        "project_root = Path().resolve().parent\n",
        "sys.path.append(str(project_root))\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Import our refactored modules\n",
        "from src.utils import configure_gpu, print_memory_stats, collect_files\n",
        "from src.data import (\n",
        "    load_dicom, load_nifti, process_volume, \n",
        "    create_dataloaders, analyze_dataset_statistics_efficiently\n",
        ")\n",
        "from src.models import BaseAutoencoder, VAE\n",
        "from src.training import (\n",
        "    TrainingConfig, VAEConfig, \n",
        "    train_autoencoder, train_vae\n",
        ")\n",
        "from src.analysis import (\n",
        "    plot_training_history, plot_vae_training_history,\n",
        "    visualize_reconstruction_samples, visualize_vae_reconstructions,\n",
        "    extract_latent_vectors, visualize_latent_space,\n",
        "    evaluate_model_performance\n",
        ")\n",
        "\n",
        "# Configure plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"âœ… All imports successful!\")\n",
        "print(f\"Project root: {project_root}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Configure GPU and Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure GPU automatically\n",
        "device = configure_gpu()\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Print memory statistics\n",
        "print_memory_stats()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Data Loading and Exploration {#data}\n",
        "\n",
        "### Data Collection and Metadata Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Collect medical image files\n",
        "# Replace with your actual data directory\n",
        "data_dir = \"/path/to/your/medical/images\"\n",
        "\n",
        "# Collect files (supports DICOM and NIfTI)\n",
        "# files = collect_files(data_dir, extensions=['.dcm', '.nii', '.nii.gz'])\n",
        "\n",
        "# For demonstration, let's create a sample metadata CSV\n",
        "sample_data = {\n",
        "    'file_path': [f'sample_{i}.nii.gz' for i in range(100)],\n",
        "    'group': ['Control' if i < 50 else 'Patient' for i in range(100)],\n",
        "    'age': np.random.normal(65, 10, 100),\n",
        "    'gender': np.random.choice(['M', 'F'], 100)\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(sample_data)\n",
        "print(f\"Created sample dataset with {len(df)} subjects\")\n",
        "print(\"\\nDataset overview:\")\n",
        "print(df.head())\n",
        "print(\"\\nGroup distribution:\")\n",
        "print(df['group'].value_counts())\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Model Training {#training}\n",
        "\n",
        "### 3.1 Autoencoder Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create autoencoder model\n",
        "input_shape = (64, 128, 128)\n",
        "latent_dim = 256\n",
        "\n",
        "autoencoder = BaseAutoencoder(\n",
        "    input_shape=input_shape,\n",
        "    latent_dim=latent_dim\n",
        ").to(device)\n",
        "\n",
        "print(f\"Autoencoder created:\")\n",
        "print(f\"  Input shape: {input_shape}\")\n",
        "print(f\"  Latent dimension: {latent_dim}\")\n",
        "print(f\"  Total parameters: {sum(p.numel() for p in autoencoder.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure training\n",
        "ae_config = TrainingConfig(\n",
        "    epochs=50,\n",
        "    batch_size=4,\n",
        "    learning_rate=0.001,\n",
        "    optimizer='adam',\n",
        "    scheduler='reduce_on_plateau',\n",
        "    loss_function='mse',\n",
        "    device=str(device),\n",
        "    use_amp=True,\n",
        "    early_stopping_patience=10,\n",
        "    save_every=10,\n",
        "    checkpoint_dir='checkpoints',\n",
        "    model_name='demo_autoencoder'\n",
        ")\n",
        "\n",
        "print(\"Training configuration:\")\n",
        "print(f\"  Epochs: {ae_config.epochs}\")\n",
        "print(f\"  Learning rate: {ae_config.learning_rate}\")\n",
        "print(f\"  Optimizer: {ae_config.optimizer}\")\n",
        "print(f\"  Mixed precision: {ae_config.use_amp}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 3.2 VAE Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create VAE model\n",
        "vae = VAE(\n",
        "    input_shape=input_shape,\n",
        "    latent_dim=latent_dim\n",
        ").to(device)\n",
        "\n",
        "print(f\"VAE created:\")\n",
        "print(f\"  Input shape: {input_shape}\")\n",
        "print(f\"  Latent dimension: {latent_dim}\")\n",
        "print(f\"  Total parameters: {sum(p.numel() for p in vae.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure VAE training\n",
        "vae_config = VAEConfig(\n",
        "    epochs=60,\n",
        "    batch_size=4,\n",
        "    learning_rate=0.001,\n",
        "    optimizer='adam',\n",
        "    scheduler='reduce_on_plateau',\n",
        "    device=str(device),\n",
        "    use_amp=True,\n",
        "    beta=1.0,\n",
        "    beta_warmup_steps=10,\n",
        "    free_bits=0.0,\n",
        "    reconstruction_loss='mse',\n",
        "    early_stopping_patience=15,\n",
        "    save_every=10,\n",
        "    checkpoint_dir='checkpoints',\n",
        "    model_name='demo_vae'\n",
        ")\n",
        "\n",
        "print(\"VAE training configuration:\")\n",
        "print(f\"  Beta (KL weight): {vae_config.beta}\")\n",
        "print(f\"  Beta warmup steps: {vae_config.beta_warmup_steps}\")\n",
        "print(f\"  Free bits: {vae_config.free_bits}\")\n",
        "print(f\"  Reconstruction loss: {vae_config.reconstruction_loss}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Analysis and Visualization {#analysis}\n",
        "\n",
        "Note: The following cells demonstrate how to use the analysis functions once you have trained models and real data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Training with data loaders (uncomment when you have real data)\n",
        "# train_loader, val_loader = create_dataloaders(\n",
        "#     df=df,\n",
        "#     data_dir=data_dir,\n",
        "#     batch_size=4,\n",
        "#     train_split=0.8,\n",
        "#     val_split=0.2,\n",
        "#     target_shape=(64, 128, 128),\n",
        "#     num_workers=2\n",
        "# )\n",
        "\n",
        "# # Train autoencoder\n",
        "# ae_history = train_autoencoder(\n",
        "#     model=autoencoder,\n",
        "#     train_loader=train_loader,\n",
        "#     val_loader=val_loader,\n",
        "#     config=ae_config\n",
        "# )\n",
        "\n",
        "# # Train VAE\n",
        "# vae_history = train_vae(\n",
        "#     model=vae,\n",
        "#     train_loader=train_loader,\n",
        "#     val_loader=val_loader,\n",
        "#     config=vae_config\n",
        "# )\n",
        "\n",
        "print(\"ðŸ’¡ Training code ready - uncomment when you have medical image data\")\n",
        "print(\"ðŸ’¡ The models will train on your DICOM/NIfTI files automatically\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Visualization functions (uncomment after training)\n",
        "# plot_training_history(ae_history, save_path='autoencoder_history.png')\n",
        "# plot_vae_training_history(vae_history, save_path='vae_history.png')\n",
        "\n",
        "# # Visualize reconstructions\n",
        "# visualize_reconstruction_samples(\n",
        "#     model=autoencoder,\n",
        "#     dataloader=val_loader,\n",
        "#     num_samples=3,\n",
        "#     save_path='autoencoder_reconstructions.png'\n",
        "# )\n",
        "\n",
        "# visualize_vae_reconstructions(\n",
        "#     model=vae,\n",
        "#     dataloader=val_loader,\n",
        "#     num_samples=3,\n",
        "#     save_path='vae_reconstructions.png'\n",
        "# )\n",
        "\n",
        "print(\"ðŸ’¡ Visualization functions ready for trained models\")\n",
        "print(\"ðŸ’¡ Will generate training curves, reconstruction comparisons, and more\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Latent space analysis (uncomment after VAE training)\n",
        "# latent_vectors, group_labels = extract_latent_vectors(\n",
        "#     model=vae,\n",
        "#     dataloader=val_loader,\n",
        "#     include_groups=True\n",
        "# )\n",
        "\n",
        "# # Visualize latent space with t-SNE\n",
        "# visualize_latent_space(\n",
        "#     latent_vectors=latent_vectors,\n",
        "#     labels=group_labels,\n",
        "#     method='tsne',\n",
        "#     save_path='latent_space_tsne.png'\n",
        "# )\n",
        "\n",
        "# # Visualize with PCA\n",
        "# visualize_latent_space(\n",
        "#     latent_vectors=latent_vectors,\n",
        "#     labels=group_labels,\n",
        "#     method='pca',\n",
        "#     save_path='latent_space_pca.png'\n",
        "# )\n",
        "\n",
        "print(\"ðŸ’¡ Latent space analysis ready\")\n",
        "print(\"ðŸ’¡ Will generate t-SNE and PCA plots to visualize learned representations\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Advanced Usage {#advanced}\n",
        "\n",
        "### Command Line Interface\n",
        "\n",
        "The refactored codebase provides powerful CLI scripts:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\"\"\n",
        "Command-Line Usage Examples:\n",
        "\n",
        "1. Train Autoencoder:\n",
        "   python scripts/train_autoencoder.py \\\\\n",
        "     --data_csv data.csv \\\\\n",
        "     --data_dir /path/to/images \\\\\n",
        "     --epochs 100 \\\\\n",
        "     --batch_size 4 \\\\\n",
        "     --latent_dim 256\n",
        "\n",
        "2. Train VAE:\n",
        "   python scripts/train_vae.py \\\\\n",
        "     --data_csv data.csv \\\\\n",
        "     --data_dir /path/to/images \\\\\n",
        "     --epochs 150 \\\\\n",
        "     --beta 1.0 \\\\\n",
        "     --beta_warmup_steps 20\n",
        "\n",
        "3. Evaluate Model:\n",
        "   python scripts/evaluate_model.py \\\\\n",
        "     --checkpoint_path model_best.pth \\\\\n",
        "     --model_type autoencoder \\\\\n",
        "     --data_csv data.csv \\\\\n",
        "     --data_dir /path/to/images \\\\\n",
        "     --visualize \\\\\n",
        "     --latent_analysis\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Configuration Files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and display configuration files\n",
        "import json\n",
        "\n",
        "# Load default autoencoder config\n",
        "with open(project_root / 'configs' / 'autoencoder_default.json', 'r') as f:\n",
        "    ae_config_dict = json.load(f)\n",
        "\n",
        "print(\"Default Autoencoder Configuration:\")\n",
        "print(json.dumps(ae_config_dict, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load VAE config\n",
        "with open(project_root / 'configs' / 'vae_default.json', 'r') as f:\n",
        "    vae_config_dict = json.load(f)\n",
        "\n",
        "print(\"Default VAE Configuration:\")\n",
        "print(json.dumps(vae_config_dict, indent=2))\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates the complete workflow of the refactored medical image analysis codebase:\n",
        "\n",
        "### Key Features:\n",
        "1. **Modular Design**: Clean separation between data, models, training, and analysis\n",
        "2. **Medical Image Support**: DICOM and NIfTI format handling with preprocessing\n",
        "3. **Advanced Models**: 3D Autoencoders and VAEs with configurable architectures\n",
        "4. **Training Infrastructure**: Comprehensive training with callbacks, checkpointing, and early stopping\n",
        "5. **Analysis Tools**: Visualization, evaluation, and interpretation functions\n",
        "6. **CLI Support**: Command-line scripts for training and evaluation\n",
        "7. **Configuration Management**: JSON-based configuration files\n",
        "\n",
        "### Next Steps:\n",
        "1. Prepare your medical image dataset (DICOM/NIfTI files)\n",
        "2. Create metadata CSV file with file paths and groups\n",
        "3. Modify configurations as needed\n",
        "4. Train models using this notebook or CLI scripts\n",
        "5. Analyze results using the provided tools\n",
        "\n",
        "### Project Structure:\n",
        "```\n",
        "â”œâ”€â”€ src/                    # Core modules\n",
        "â”‚   â”œâ”€â”€ data/              # Data loading and preprocessing\n",
        "â”‚   â”œâ”€â”€ models/            # Model architectures\n",
        "â”‚   â”œâ”€â”€ training/          # Training infrastructure\n",
        "â”‚   â”œâ”€â”€ analysis/          # Analysis and visualization\n",
        "â”‚   â””â”€â”€ utils/             # Utility functions\n",
        "â”œâ”€â”€ scripts/               # CLI scripts\n",
        "â”œâ”€â”€ configs/               # Configuration files\n",
        "â”œâ”€â”€ notebooks/             # Demo notebooks\n",
        "â””â”€â”€ requirements.txt       # Dependencies\n",
        "```\n",
        "\n",
        "The refactored codebase transforms a monolithic 46MB notebook into a maintainable, extensible, and production-ready framework for medical image analysis research.\n",
        "\n",
        "**ðŸŽ¯ Ready to analyze your medical images with state-of-the-art deep learning!**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
